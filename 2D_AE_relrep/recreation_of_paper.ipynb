{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility and consistency across runs, we will set a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "def set_random_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_random_seeds()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))  # Prints the GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(batch_size=64, download=True):\n",
    "    \"\"\"\n",
    "    Loads and returns MNIST train and test DataLoaders.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): The batch size for the DataLoader.\n",
    "        download (bool): Whether to download the dataset if not found.\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoader objects for the MNIST dataset.\n",
    "    \"\"\"\n",
    "    # Define a transform to normalize the data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load the training and test datasets\n",
    "    train_dataset = datasets.MNIST(root='../datasets', train=True, transform=transform, download=download) #'..' to refer back to motherfolder\n",
    "    test_dataset = datasets.MNIST(root='../datasets', train=False, transform=transform, download=download)\n",
    "    \n",
    "    # Create DataLoaders for the datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load the MNIST data\n",
    "train_loader, test_loader = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder with a bottleneck of size 2 that maps MNIST images to a 2D latent space.\n",
    "    Includes training, evaluation, and embedding extraction methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### NOTES ###\n",
    "    # Might have to use batchnorm to impose a structure on the latent space\n",
    "\n",
    "    def __init__(self, latent_dim=2, hidden_size=128, use_batchnorm=True):\n",
    "        super().__init__()\n",
    "        # Encoder layers\n",
    "        encoder_layers = [\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, hidden_size),\n",
    "            nn.ReLu()\n",
    "        ]\n",
    "        if use_batchnorm:\n",
    "            encoder_layers.append(nn.Batchnorm1d(hidden_size))\n",
    "        encoder_layers.append(nn.Linear(hidden_size, latent_dim)) # the size 2 bottleneck layer\n",
    "        self.encoder = nn.Sequential(*encoder_layers) # '*' is unpacking the list into it's elements\n",
    "\n",
    "        pass\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encodes an input batch (e.g., MNIST images) into the latent space.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input images of shape [batch_size, 1, 28, 28].\n",
    "        Returns:\n",
    "            latents (Tensor): Encoded latent vectors of shape [batch_size, latent_dim].\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes latent vectors back to the original image space.\n",
    "        \n",
    "        Args:\n",
    "            z (Tensor): Latent vectors of shape [batch_size, latent_dim].\n",
    "        Returns:\n",
    "            reconstructed (Tensor): Reconstructed images of shape [batch_size, 1, 28, 28].\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Complete forward pass: encode then decode.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input images.\n",
    "        Returns:\n",
    "            reconstructed (Tensor): Reconstructed images of the same shape as x.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_one_epoch(self, train_loader, optimizer, criterion, device='cpu'):\n",
    "        \"\"\"\n",
    "        Performs one epoch of training.\n",
    "        \n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for the training set.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
    "            criterion: Loss function (e.g., MSELoss, BCELoss).\n",
    "            device (str): 'cpu' or 'cuda' device.\n",
    "        \n",
    "        Returns:\n",
    "            epoch_loss (float): Average loss across this training epoch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, data_loader, criterion, device='cpu'):\n",
    "        \"\"\"\n",
    "        Evaluates the autoencoder on a given dataset (test or validation).\n",
    "        \n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the evaluation set.\n",
    "            criterion: Loss function for reconstruction.\n",
    "            device (str): 'cpu' or 'cuda'.\n",
    "        \n",
    "        Returns:\n",
    "            eval_loss (float): Average reconstruction loss on this dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, train_loader, test_loader, num_epochs, lr=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        High-level method to train the autoencoder for a given number of epochs.\n",
    "        It orchestrates optimizer setup, training loop, and evaluation per epoch.\n",
    "        \n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for training set.\n",
    "            test_loader (DataLoader): DataLoader for test/validation set.\n",
    "            num_epochs (int): Number of epochs.\n",
    "            lr (float): Learning rate for the optimizer.\n",
    "            device (str): 'cpu' or 'cuda'.\n",
    "        \n",
    "        Returns:\n",
    "            train_losses (list of float): Loss for each training epoch.\n",
    "            test_losses (list of float): Loss for each test epoch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_latent_embeddings(self, data_loader, device='cpu'):\n",
    "        \"\"\"\n",
    "        Passes the entire dataset through the encoder to extract latent vectors.\n",
    "        \n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the dataset to encode.\n",
    "            device (str): 'cpu' or 'cuda'.\n",
    "        \n",
    "        Returns:\n",
    "            embeddings (Tensor): Concatenated latent vectors of shape [N, latent_dim].\n",
    "            labels (Tensor): Corresponding labels (if available) of shape [N].\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_anchors(embeddings, num_anchors=10):\n",
    "    \"\"\"\n",
    "    Selects a subset of embeddings to use as 'anchors' for relative representation.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (Tensor or array): Shape [N, latent_dim].\n",
    "        num_anchors (int): Number of anchors to select.\n",
    "    \n",
    "    Returns:\n",
    "        anchors: A (num_anchors, latent_dim) subset of the original embeddings.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_relative_coordinates(embeddings, anchors):\n",
    "    \"\"\"\n",
    "    Transforms 'embeddings' into a 'relative' coordinate system based on anchors.\n",
    "    One approach could be subtracting an anchor or computing offsets.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (Tensor): Shape [N, latent_dim].\n",
    "        anchors (Tensor): Shape [A, latent_dim], where A = num_anchors.\n",
    "    \n",
    "    Returns:\n",
    "        relative_embeds (Tensor): The embeddings expressed relative to the anchors.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_epochs=5, batch_size=64, lr=1e-3, device='cpu', latent_dim=2):\n",
    "    \"\"\"\n",
    "    Orchestrates the autoencoder pipeline:\n",
    "      1. Load data\n",
    "      2. Initialize the autoencoder\n",
    "      3. Train and evaluate\n",
    "      4. Extract embeddings\n",
    "      5. (Optional) Select anchors, compute relative coordinates\n",
    "    \n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        batch_size (int): DataLoader batch size.\n",
    "        lr (float): Learning rate.\n",
    "        device (str): 'cpu' or 'cuda' device.\n",
    "        latent_dim (int): Dimension of the AE's latent space (2 for easy visualization).\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained autoencoder.\n",
    "        embeddings (Tensor): Latent embeddings from the test (or train) set.\n",
    "        anchors (Tensor): (Optional) set of anchor embeddings if you implement that step here.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
