{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework for next time:\n",
    "\n",
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12/02/2025\n",
    "#### **Meeting - Distance measures?**\n",
    "\n",
    "##### **Plan for next time**\n",
    "\n",
    "- Write down thoughts about the project, distance measures, P matrix, etc. (see notes below)\n",
    "\n",
    "- Look through the Overleaf for the other special course\n",
    "\n",
    "- Do explorative analysis with P matrix, noise as robustness, and using euclidian measure with BachNorm\n",
    "\n",
    "##### **Notes**\n",
    "\n",
    "- The diversity measure will be larger due to amount of points\n",
    "\n",
    "- A = X * P\n",
    "\n",
    "    - Width of P is #A, and P is picking anchors from X (only one 1 per row)\n",
    "\n",
    "    - Either softmax p (sum to 1) - Then we have linear combinations of X as anchors\n",
    "\n",
    "    - Or we take the highest value of another matrix J (sampling from J) columnwise to get P\n",
    "        - Maybe this isn't optimal as we end up optimizing only for coverage (rewrite cov and div as depending on P to see)\n",
    "\n",
    "        L1 los\n",
    "\n",
    "- Why shouldnt we want to compute iteratively?\n",
    "\n",
    "    - As long as we have a good enough first point and the #A doesnt execed #D, then it shold be ok. Make sure to place the first point properly\n",
    "\n",
    "    - However, still takes a long time computationally\n",
    "\n",
    "- Coverage vs div\n",
    "\n",
    "- How do we measure at all?\n",
    "\n",
    "    - Combining measures? Bad practice\n",
    "\n",
    "    - Robustness in loss func. noise?\n",
    "\n",
    "    - Try using L2 to find the anchors \n",
    "\n",
    "Freeze encoder, train decoder on relrep and compare loss\n",
    "\n",
    "Just use euclidian??????? (first invariance to scaling with batchnorm)\n",
    "\n",
    "nn.Parameter(Tensor) (for when finding A with P. Set of random points into nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09/02/2025\n",
    "## Homework for next time:\n",
    "\n",
    "N/A\n",
    "#### **Relreps in higher dim and streamlining code**\n",
    "\n",
    "- Experimenting with higher dimensional embeddings and how this translates visually to relative representations using PCA (and 2D anchors).\n",
    "\n",
    "- Refining the AE class and dataloader to include indexes for each image in a way where we can always match parallel points (incl. anchors) in different latent spaces trained on the same data.\n",
    "\n",
    "- Created a similarity function for quantitative similarity measures between embeddings (MRR & cosine)\n",
    "\n",
    "- Initial work on the greedy optimization function for anchors\n",
    "\n",
    "#### **Results**\n",
    "<img src=\"experiments/initial_AE_testing/embeddings.png\" alt=\"2D Latent Encodings\" width=\"85%\">\n",
    "\n",
    "\n",
    "<img src=\"experiments/initial_AE_testing/rel_reps.png\" alt=\"2D Latent Encodings\" width=\"85%\">\n",
    "\n",
    "$\\textbf{Interesting observation}$: Building on the previous hypothesis: We have confirmed (visually) that the relative representation will always be a n-dimensional hyper-sphere where n is the amount of anchors if n >= the latent embedding size.\n",
    "\n",
    "##### **Plan for next time**\n",
    "- Organize codebase\n",
    "\n",
    "- Prepare for meeting\n",
    "\n",
    "- Get similarity functions up and running\n",
    "\n",
    "- Continue work on greedy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06/02/2025\n",
    "#### **Recreating relreps paper results in 2D using MNIST**\n",
    "\n",
    "- Creating the AE class\n",
    "- Creating functions to select anchors at random and comput relative coordinates\n",
    "- Running tests and plotting\n",
    "- Making sure that we can create embeddings with new seeds while maintaining the same anchors\n",
    "\n",
    "#### Results\n",
    "<img src=\"experiments/initial_AE_testing/2D_latent_encodings.png\" alt=\"2D Latent Encodings\" width=\"30%\">\n",
    "\n",
    "first plot of relative representation. made from a 2D latent space using 2 anchors\n",
    "\n",
    "<img src=\"experiments/initial_AE_testing/first_rel_rep_plot.png\" alt=\"2D Latent Encodings\" width=\"30%\">\n",
    "\n",
    "$\\textbf{Interesting observation}$: Relative representations will always create a \"hyper-ellipse\" in some n-dimensional vector space where n is equal to the number of anchors. Because you can't change one dimension without changing the others. It is easiest to visualize in 2D, but the same rule applies for higher dimensions. Maybe this is useful somehow.\n",
    "##### **Plan for next time**\n",
    "- Try making a AE with higher dimensional output and making relative representation on that\n",
    "- Experiment with new ways of choosing anchors\n",
    "- Compare absolute and relative latent spaces between different latent spaces of MNIST\n",
    "- Experiment with high-dimensional embeddings\n",
    "    - PCA for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATE\n",
    "#### **TITLE**\n",
    "\n",
    "-\n",
    "\n",
    "#### **Results**\n",
    "<img src=\"experiments/initial_AE_testing/embeddings.png\" alt=\"2D Latent Encodings\" width=\"100%\">\n",
    "\n",
    "$\\textbf{Interesting observation}$: \n",
    "\n",
    "##### **Plan for next time**\n",
    "\n",
    "\n",
    "##### **Questions**\n",
    "\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
